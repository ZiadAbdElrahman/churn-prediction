# Churn Prediction System

This project implements an end-to-end machine learning pipeline for churn prediction, with **model training, evaluation, experiment tracking, and serving** in production.

It combines:
- **XGBoost classifier** with configurable hyperparameters  
- **MLflow** for experiment tracking and logging  
- **FastAPI** service for real-time predictions and training endpoint  
- **Docker** and **uv** for reproducible builds and environments  

---

## ğŸš€ Features

- **Training**
  - Train an XGBoost churn model with configurable hyperparameters.
  - Logs metrics (AUC-PR, logloss, classification report) and artifacts to MLflow.
  - Stores model, threshold, and metadata under `artifacts/`.

- **Serving**
  - `/train` endpoint to retrain the model on demand.
  - `/predict` endpoint for batch or single predictions.

---

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                     # Core ML logic
â”‚   â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”‚   â”œâ”€â”€ training_pipeline.py  # Full training pipeline
â”‚   â”‚   â”œâ”€â”€ data_access.py        # Data loading & preprocessing
â”‚   â”‚   â”œâ”€â”€ features.py           # Feature engineering
â”‚   â”‚   â”œâ”€â”€ model_io.py           # Save/load model artifacts
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ engine/                   # Business / model engine layer
â”‚   â”‚   â””â”€â”€ controller/           # Controller logic
â”‚   â”‚       â”œâ”€â”€ churn_detection.py
â”‚   â”‚       â”œâ”€â”€ types.py
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ service/                  # API service layer
â”‚   â”‚   â”œâ”€â”€ main.py               # FastAPI entrypoint
â”‚   â”‚   â”œâ”€â”€ churn_detection.py    # API endpoints
â”‚   â”‚   â”œâ”€â”€ dependencies.py       # Config & shared deps
â”‚   â”‚   â”œâ”€â”€ types.py              # Request/response schemas
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml               # Global configuration
â”‚
â”œâ”€â”€ data/                         # Raw and reference data
â”‚   â”œâ”€â”€ put_data_here             # Placeholder
â”‚   â”œâ”€â”€ customer_churn.json       # Full dataset (large)
â”‚   â””â”€â”€ customer_churn_mini.json  # Mini dataset for testing
â”‚
â”œâ”€â”€ artifacts/                    # Models, metrics, logs
â”œâ”€â”€ notebooks/                    # Experimentation notebooks
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml       # Code quality hooks (ruff, black, etc.)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile                      # Reproducible commands
â”œâ”€â”€ pyproject.toml                # Dependencies & build config (uv / PEP 621)
â”œâ”€â”€ README.md
```

---



## âš™ï¸ Installation

Clone the repository:

```bash
git clone https://github.com/<your-username>/churn-prediction.git
cd churn-prediction
```

## ğŸ“‚ Dataset
Before running training or serving, you need to download the data file in the /data folder:
```bash
customer_churn.json
```

### Build and Run with Docker + Make

Build the Docker image:
```bash
make build
```

Run the API service:
```bash
make run
```

The service will be available at: http://localhost:8000


## ğŸ—ï¸ Usage

### 1. Train a model
You can train via API:
```bash
curl -X POST "http://0.0.0.0:8000/api/v1/train" \
  -H "Content-Type: application/json"
```

### 2. Inference
You can run via API:
```bash
curl -X POST "http://0.0.0.0:8000/api/v1/predict" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -d '{
  "user_id": "100024",
  "date": "2018-10-08 21:04:57"
}'
```

---

## ğŸ“Š MLflow Tracking

Each training run logs:
- Metrics (PR-AUC, recall, precision)
- Parameters (XGBoost hyperparams)
- Artifacts (model, reports)

Run UI:
```bash
mlflow ui
```

---

## ğŸ§¹ Development

This repo uses **pre-commit hooks** for code quality.

Install hooks:
```bash
pre-commit install
```

Run checks:
```bash
pre-commit run --all-files
```

---

## ğŸ“Œ TODO / Extensions

- Add monitoring system:
  - **Data drift** detection (PSI, KS test per feature).  
  - **Concept drift** detection (performance decay).  
  - Automated performance tracking.  
- Integrate retraining job on severe drift.  
- Integrate alerting (Slack/Email).  
- Add dashboard for monitoring visualization.  

---
